# ===============================================
# POR QUE PYDOOP FALHOU E O QUE REALMENTE RESOLVER
# ===============================================
# %%
"""
‚ùå ERRO NO PYDOOP - CAUSAS:
==========================
1. Python 3.11 (muito novo para pydoop 2.0.0)
2. pydoop precisa compilar c√≥digo C++ (complexo no Windows)
3. pydoop precisa Hadoop COMPLETO instalado (n√£o s√≥ winutils)
4. pydoop N√ÉO resolve o erro NativeIO do Spark
"""

import sys
import platform

def explicar_erro_pydoop():
    """
    Explica por que pydoop falhou e por que n√£o √© a solu√ß√£o
    """
    
    print("üö´ POR QUE PYDOOP FALHOU")
    print("="*40)
    
    print(f"Python vers√£o: {sys.version}")
    print(f"Sistema: {platform.system()} {platform.release()}")
    
    print("""
    PROBLEMAS IDENTIFICADOS:
    ========================
    
    1. üêç PYTHON 3.11 INCOMPAT√çVEL:
       - pydoop 2.0.0 foi feito para Python <= 3.9
       - Setuptools 80.9.0 muito novo
       - AttributeError: 'NoneType' object has no attribute 'strip'
    
    2. üîß COMPILA√á√ÉO C++:
       - pydoop precisa compilar extens√µes C++
       - Windows n√£o tem compilador por padr√£o
       - Precisa Microsoft C++ Build Tools
    
    3. üêò HADOOP COMPLETO NECESS√ÅRIO:
       - pydoop precisa Hadoop COMPLETO (n√£o s√≥ winutils)
       - Precisa HDFS rodando
       - Configura√ß√µes complexas
    
    4. ‚ùå N√ÉO RESOLVE SEU PROBLEMA:
       - pydoop √© para ACESSAR dados no HDFS
       - Seu erro √© SALVAR arquivos localmente no Windows
       - S√£o problemas diferentes!
    """)

def solucoes_que_realmente_funcionam():
    """
    Solu√ß√µes que realmente resolvem o problema NativeIO
    """
    
    print("\n‚úÖ SOLU√á√ïES QUE REALMENTE FUNCIONAM")
    print("="*50)
    
    # SOLU√á√ÉO 1: Spark configurado corretamente
    print("1. üéØ SPARK COM CONFIGURA√á√ïES CERTAS:")
    print("""
    from pyspark.sql import SparkSession
    from pyspark.conf import SparkConf
    
    conf = SparkConf() \\
        .set("spark.hadoop.io.native.lib.available", "false") \\
        .set("spark.sql.warehouse.dir", "file:///C:/tmp/spark-warehouse") \\
        .set("spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version", "2")
    
    spark = SparkSession.builder.config(conf=conf).getOrCreate()
    """)
    
    # SOLU√á√ÉO 2: Fallback para Pandas
    print("\n2. üêº FALLBACK AUTOM√ÅTICO PARA PANDAS:")
    print("""
    def salvar_seguro(df, nome):
        try:
            df.write.csv(nome)
            print("Salvo com Spark!")
        except:
            df.toPandas().to_csv(f"{nome}.csv")
            print("Salvo com Pandas!")
    """)
    
    # SOLU√á√ÉO 3: Docker
    print("\n3. üê≥ DOCKER (100% FUNCIONA):")
    print("""
    # Um comando resolve tudo:
    docker run -it --rm -p 8888:8888 jupyter/pyspark-notebook
    
    # Acesse: http://localhost:8888
    # Spark + Hadoop funcionando perfeitamente!
    """)
    
    # SOLU√á√ÉO 4: WSL2
    print("\n4. üêß WSL2 (LINUX NO WINDOWS):")
    print("""
    wsl --install Ubuntu
    # Depois no Ubuntu:
    sudo apt install openjdk-8-jdk python3-pip
    pip3 install pyspark
    # Zero problemas NativeIO!
    """)

def exemplo_pratico():
    """
    Exemplo pr√°tico que funciona AGORA
    """
    
    print("\nüöÄ C√ìDIGO QUE FUNCIONA AGORA")
    print("="*40)
    
    codigo_exemplo = '''
from pyspark.sql import SparkSession
from pyspark.conf import SparkConf
import tempfile
import os

# Configura√ß√£o anti-NativeIO
conf = SparkConf().setAppName("SemProblemas") \\
    .set("spark.hadoop.io.native.lib.available", "false") \\
    .set("spark.sql.warehouse.dir", f"file:///{tempfile.gettempdir()}/spark") \\
    .set("spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version", "2") \\
    .set("spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored", "true")

spark = SparkSession.builder.config(conf=conf).getOrCreate()

# Seus dados
dados = [("Jo√£o", 25), ("Maria", 30), ("Pedro", 28)]
df = spark.createDataFrame(dados, ["nome", "idade"])

print("DataFrame criado:")
df.show()

# Fun√ß√£o que SEMPRE funciona
def salvar_garantido(dataframe, nome_arquivo):
    try:
        # Tentar Spark primeiro
        dataframe.coalesce(1).write.mode("overwrite").csv(nome_arquivo)
        print(f"‚úÖ Salvo com Spark: {nome_arquivo}")
    except Exception as e:
        print(f"‚ö†Ô∏è Spark falhou: {str(e)[:50]}...")
        try:
            # Fallback para Pandas
            pandas_df = dataframe.toPandas()
            pandas_df.to_csv(f"{nome_arquivo}.csv", index=False)
            print(f"‚úÖ Salvo com Pandas: {nome_arquivo}.csv")
        except Exception as e2:
            print(f"‚ùå Tudo falhou: {e2}")

# Usar a fun√ß√£o
salvar_garantido(df, "meus_dados")

spark.stop()
    '''
    
    print(codigo_exemplo)

def comandos_uteis():
    """
    Comandos √∫teis para resolver definitivamente
    """
    
    print("\nüõ†Ô∏è COMANDOS PARA RESOLVER DEFINITIVAMENTE")
    print("="*60)
    
    print("OP√á√ÉO A - Docker (Mais f√°cil):")
    print("  docker run -it --rm -p 8888:8888 jupyter/pyspark-notebook")
    
    print("\nOP√á√ÉO B - WSL2 (Para Windows):")
    print("  wsl --install Ubuntu")
    print("  # Depois instalar Spark no Ubuntu")
    
    print("\nOP√á√ÉO C - Downgrade Python (Complexo):")
    print("  # Instalar Python 3.9 ou 3.8")
    print("  # Reinstalar PySpark")
    
    print("\nOP√á√ÉO D - Usar apenas Pandas:")
    print("  pip install pandas")
    print("  # Processar dados com pandas instead")

def main():
    """Executar explica√ß√£o completa"""
    
    explicar_erro_pydoop()
    solucoes_que_realmente_funcionam()
    exemplo_pratico()
    comandos_uteis()
    
    print("\n" + "="*60)
    print("üéØ RESUMO:")
    print("- pydoop N√ÉO resolve seu problema")
    print("- Use as configura√ß√µes Spark que mostrei")
    print("- Docker √© a solu√ß√£o mais simples")
    print("- WSL2 √© a solu√ß√£o mais permanente")
    print("="*60)

if __name__ == "__main__":
    main()

# TESTE R√ÅPIDO: rode isso para ver se Spark funciona
def teste_rapido():
    try:
        from pyspark.sql import SparkSession
        spark = SparkSession.builder.appName("teste").getOrCreate()
        df = spark.createDataFrame([("teste", 1)], ["col", "val"])
        df.show()
        spark.stop()
        print("‚úÖ Spark funcionando!")
        return True
    except Exception as e:
        print(f"‚ùå Spark com problema: {e}")
        return False

# Descomente para testar:
# teste_rapido()
# %%
